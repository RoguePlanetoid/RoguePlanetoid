<!doctype html>
<html lang="en" class="h-100">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=yes">
        <meta name="description" content="RoguePlanetoid"/>
        <!-- Twitter -->
        <meta name="twitter:card" content="summary_large_image"/>
        <meta name="twitter:site" content="@rogueplanetoid"/>
        <meta name="twitter:creator" content="@rogueplanetoid"/>
        <meta name="twitter:title" content="RoguePlanetoid"/>
        <meta name="twitter:description" content="Posts, Articles and Podcast"/>
        <meta name="twitter:image" content="https://www.rogueplanetoid.com/images/card.png"/>
        <meta name="twitter:image:alt" content="Posts, Articles and Podcast"/>
        <!-- /Twitter -->
        <!-- Open Graph -->
        <meta property="og:type" content="website"/>
        <meta property="og:title" content="RoguePlanetoid"/>
        <meta property="og:site_name" content="RoguePlanetoid">
        <meta property="og:description" content="Posts, Articles and Podcast"/>
        <meta property="og:image" content="https://www.rogueplanetoid.com/images/card.png"/>
        <meta property="og:image:width" content="1280"/>
        <meta property="og:image:height" content="630"/>
        <!-- /Open Graph -->
        <link rel="icon" href="../../favicon.ico"/>
        <link href="../../css/style.css" rel="stylesheet">
        <link href="../../bootstrap/css/bootstrap.min.css" rel="stylesheet">
        <title>RoguePlanetoid - Articles - DDD North 2024</title>
        <!-- Google tag (gtag.js) -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=G-8038JPY7YN"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());
            gtag('config', 'G-8038JPY7YN');
        </script>
    </head>
    <body class="d-flex flex-column h-100">
        <!-- Main -->
        <main>       
            <!-- Navigation -->	
            <nav class="navbar top navbar-light navbar-expand-md bg-brand">
                <div class="container-fluid">
                    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#rogueplanetoid-navbar" aria-controls="rogueplanetoid-navbar" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                    </button>
                    <a class="navbar-brand" href="../../">
                        <img src="../../images/header-articles.png" alt="RoguePlanetoid" height="25px">
                    </a>
                    <div id="rogueplanetoid-navbar" class="collapse navbar-collapse">
                        <ul class="navbar-nav">
                            <li class="nav-item">
                                <a class="nav-link text-articles" href="../../">Home</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link text-articles" href="../../posts">Posts</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link text-articles" href="../../articles">Articles</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link btn btn-articles active" href="../ddd-north-2024/">DDD North 2024</a>
                            </li>
                            <li class="nav-item">
                                <a class="nav-link text-articles" href="../../podcast">Podcast</a>
                            </li>
                        </ul>
                    </div>
                    <a href="https://www.comentsys.com" target="_blank">
                        <img src="../../images/comentsys.png" alt="Comentsys" height="50px">
                    </a>
                </div>
            </nav>
            <!-- /Navigation -->     
            <!-- Section -->               
            <section class="text-center text-black container">
                <!-- Article -->
                <div class="text-start">
                    <article class="blog-post">
                        <h2 class="blog-post-title mb-1">DDD North 2024</h2>
                        <p class="blog-post-meta">2nd March 2022</p>                
                        <img src="../assets/ddd-north.png" class="img-fluid" alt="DDD North 2024"/>
                        <p>
                            <a href="https://www.dddnorth.co.uk/" target="_blank">DDD North</a> held at the University of Hull on 2nd March 2024 attracted speakers 
                            and attendees from all around the country, and some from outside the country! There were many sessions covering a wide variety of subjects 
                            along with an opportunity to network with fellow developers and so much more!
                        </p>
                        <h3>Introduction to StereoKit Mixed Reality Development - Lee Englestone</h3>
                        <p>
                            Lee Englestone works for Avanade and is the Developer Relations lead there and an MVP in Developer Technologies and Mixed Reality and have written 
                            a book .NET Developer's Guide to Augmented Reality in iOS and they have also done other talks on Generative AI.
                        </p>
                        <p>
                            A few years ago Oculus before they got acquired by Meta released the Oculus 2 headset and got the widest adoption, probably because you could play 
                            games, light smashing blocks with light sabres but after playing with it for a few months you put it away. They then heard about using C# and 
                            Visual Studio to create XR experiences for Meta Quest Headsets. StereoKit is still being developed despite the teams behind Mixed Reality in 
                            Microsoft being reduced.
                        </p>
                        <p>
                            StereoKit at <a href="https://www.stereokit.net" target="_blank">stereokit.net</a> is a mixed reality framework for Visual Studio and C# and it supports OpenXR and can deploy it with minimal changes to Quest 1, Quest 2 
                            or even HoloLens. StereoKit also supports hands & controllers, it has more of a productivity focus so it would not be the choice for a game but has 
                            support for windows and controls in a virtual space. It has been built with minimal code in mind. You can also deploy to a simulator and when ready 
                            can then deploy to a headset and it is quick to deploy code to either a headset or simulator, Unity for example does take longer to build and deploy.
                        </p>
                        <p>
                            StereoKit supports hands with interfaces along with inking and windows with controls and can command things to come towards you, it can also support 
                            3D models and physics along with lighting sources that can be reacted to along with file system access.  
                        </p>
                        <p>
                            You will need to install the .NET Multiplatform UI workload for Visual Studio along with Android SDK and will need to enable developer mode on the 
                            Headset in the Oculus app which is in the headset settings and need to allow USB debugging so Visual Studio can access the device, you also need to 
                            install the Microsoft .NET SDK along with the StereoKit templates then it is very simple to create a project, you need to name the folder to get the 
                            name of the project using the dotnet new sk-multi which will contain the C# code and the code to deploy it to the headset in two projects. If you 
                            start the project this will launch the simulator but if do everything correctly you can also see the headset itself as a target.
                        </p>
                        <p>
                            SideQuest VR is recommended which will allow you to access the filesystem and stream things from the headset such as to OBS or YouTube and you can 
                            sideload APKs and can toggle certain developer settings. Simulator is easy to use, and you can navigate with ASDW keys and can use mouse to grab 
                            things like you would with your hand. After the first time you run your application it is there to be ran at any time on your headset so it can 
                            be run standalone without needing to be connected to Visual Studio.
                        </p>
                        <p>
                            You can write minimal code in StereoKit my just using a few lines of code to initialise StereoKit and then have a Run method runs every frame for 
                            example to create a 10 CM cube using 0.1f. A lot of thought has been put into the structure of the framework, you can use if to combine a declaration 
                            and event handler for something like a button really easily. When writing WIndowBegin you need to call WindowEnd to display a window which needs a 
                            unique ID and will be the title and then pass in the pose which is a combination of the angle and position of the window and then can have the window 
                            itself be visible or not. Controls can support Unicode and there are also built in keyboards for the text controls that are used and can also use a 
                            Hand Menu which can be interacted with for either hand or both.
                        </p>
                        <p>
                            Meta Quest 3 supports Full Colour Passthrough, the Meta Quest 2 had grainy passthrough but the later one is better, you can toggle between both unlike 
                            with HoloLens which is stuck with just augmented reality experiences. You can enable Passthrough and have a Stepper instance to do this. There is also 
                            support for Inking in 3D space which draws a bunch of lines in 3D space but can use this for different things.
                        </p>
                        <p>
                            If you have a Quest 2 or Quest 3 then you can try it out!
                        </p>
                        <h3>Supercharge your data with Azure AI Search and OpenAI - Pete Gallagher</h3>
                        <p>
                            Pete Gallagher is a full stack manager at Avanade which is part owned by Microsoft and Accenture and is a Microsoft Certified Trainer and Microsoft MVP 
                            and they organise .NET Notts. Their background is electronics and IoT but is difficult to miss AI and it is part of everyone's job now and they are 
                            also a STEM ambassador.
                        </p>
                        <p>
                            Large Language Models are trained onto a lot of information but not your own data. What is Generative AI which is a deep prediction model and will be 
                            talking about LLMs such as Gemini which has been in the new recently or OpenAI's ChatGPT and there are models such as GPT 3.5 Turbo and DALL-E etc.
                        </p>
                        <p>
                            Azure OpenID has the same models as OpenAI ChatGPT but get security, virtual networks and private links and can have responsible AI, which is 
                            opinionated as will be limited in what way you will get data back, may not be able to ask certain questions. Microsoft and Avanade are investing 
                            into responsible AI. Azure Open AI has a multi-language SDK supported.
                        </p>
                        <p>
                            A World of Unstructured Data - Text, images, video, audio etc and have AI model training and powerful prediction engines, you ask a question, and it 
                            says what the next possible token is something but doesn't know what the next one is until it says it. We need a way to being our own data into these 
                            models.
                        </p>
                        <p>
                            Retrieval Augmented Generation - We can bring our own data to the LLMs, so query based on User Query, get responses back from database and combine 
                            this with query with AI reponse and add this to the LLM.
                        </p>
                        <p>
                            Azure AI Search - Need a way to break down the data known as Chunking. Azure AI Search allows you to query your data in different ways such as text, 
                            fuzzy and vector searches on this and has a multi-language SDK so don't have to use API calls. Indexes are like a database table with schemas and 
                            fields and can set a field to be filterable and can apply a facet which is all the options for a field to populate something like a drop-down box 
                            and query over this data. You can do a full-text search, vector search and hybrid search to improve ranking of results by combining mathematical 
                            search with semantic search and can do a semantic re-ranker which can rank results better to better understand what you actually want.
                        </p>
                        <p>
                            Vector Embeddings - Convert unstructured data to numbers that AI models that can then work over, to work out the meaning of the data. Created using 
                            an AI embedding model to create a Vector Database which are designed to do vector comparison and then compare two sets of vectors and can compare 
                            these with complex mathematics to see how close the numbers that represent different parts of the data to see if things are similar. Vectors 
                            understand the semantic meaning of your data and figure out how close things are mathematically e.g. King - Man + Woman = Queen, so you get another 
                            vector and can convert this back to text for the output. Another way to think about it is to use 3D space to plot the space of these words, such as 
                            a visualisation of words that you can then do a vector search of and see words that have similar semantic meaning are nearby each other.
                        </p>
                        <p>
                            RAG in Detail - We have a front end and we connect to that to an API and connect to services such as Azure Search and Open AI, the user uploads a 
                            document which is broken up with chunking and vectors are created from the document and then can search for information using the vectors to then 
                            obtain a response based upon those documents.
                        </p>
                    </article>
                </div>
                <!-- /Article -->
            </section>
            <!-- /Section -->     
        </main>
        <!-- /Main -->
        <!-- Footer -->
        <footer class="footer mt-auto">
            <div class="steps steps-articles"></div>  
            <nav class="navbar bottom navbar-light p-2 bg-articles">
                <a href="../../"><img src="../../images/footer.png" alt="RoguePlanetoid" height="20px"></a>
                <a href="https://www.twitter.com/rogueplanetoid" target="_blank"><img src="../../images/twitter.png" height="45px" alt="@RoguePlanetoid"></a>
            </nav>
        </footer>
        <!-- /Footer -->
        <!-- Scripts -->
        <script src="../../bootstrap/js/bootstrap.bundle.min.js"></script>
        <!-- /Scripts-->
    </body>
</html>