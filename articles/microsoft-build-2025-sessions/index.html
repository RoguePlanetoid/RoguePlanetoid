<!doctype html>
<html lang="en" class="h-100">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=yes">
    <meta name="description" content="RoguePlanetoid" />
    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@rogueplanetoid" />
    <meta name="twitter:creator" content="@rogueplanetoid" />
    <meta name="twitter:title" content="RoguePlanetoid" />
    <meta name="twitter:description" content="Posts, Articles and Podcast" />
    <meta name="twitter:image" content="https://www.rogueplanetoid.com/images/card.png" />
    <meta name="twitter:image:alt" content="Posts, Articles and Podcast" />
    <!-- /Twitter -->
    <!-- Open Graph -->
    <meta property="og:type" content="website" />
    <meta property="og:title" content="RoguePlanetoid" />
    <meta property="og:site_name" content="RoguePlanetoid">
    <meta property="og:description" content="Posts, Articles and Podcast" />
    <meta property="og:image" content="https://www.rogueplanetoid.com/images/card.png" />
    <meta property="og:image:width" content="1280" />
    <meta property="og:image:height" content="630" />
    <!-- /Open Graph -->
    <link rel="icon" href="../../favicon.ico" />
    <link href="../../css/style.css" rel="stylesheet">
    <link href="../../bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <title>RoguePlanetoid - Articles - Microsoft Build 2025 - Sessions</title>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-8038JPY7YN"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-8038JPY7YN');
    </script>
</head>

<body class="d-flex flex-column h-100">
    <!-- Main -->
    <main>
        <!-- Navigation -->
        <nav class="navbar top navbar-light navbar-expand-md bg-brand">
            <div class="container-fluid">
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                    data-bs-target="#rogueplanetoid-navbar" aria-controls="rogueplanetoid-navbar" aria-expanded="false"
                    aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <a class="navbar-brand" href="../../">
                    <img src="../../images/header-articles.png" alt="RoguePlanetoid" height="25px">
                </a>
                <div id="rogueplanetoid-navbar" class="collapse navbar-collapse">
                    <ul class="navbar-nav">
                        <li class="nav-item">
                            <a class="nav-link text-articles" href="../../">Home</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link text-articles" href="../../posts">Posts</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link text-articles" href="../../articles">Articles</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link btn btn-articles active" href="../microsoft-build-2025-sessions">Microsoft Build 2025 - Sessions</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link text-articles" href="../../podcast">Podcast</a>
                        </li>
                    </ul>
                </div>
                <a href="https://www.comentsys.com" target="_blank">
                    <img src="../../images/comentsys.png" alt="Comentsys" height="50px">
                </a>
            </div>
        </nav>
        <!-- /Navigation -->
        <!-- Section -->
        <section class="text-center text-black container">
            <!-- Article -->
            <div class="text-start">
                <article class="blog-post">
                    <h2 class="blog-post-title mb-1">Microsoft Build 2025 - Sessions</h2>
                    <p class="blog-post-meta">21st May 2025</p>
                    <img src="../assets/microsoft-build-2025.png" class="img-fluid" alt="Microsoft Build 2025 - Sessions" />
                    <h3>Azure AI Foundry: The AI app and Agent Factory - Yina Arenas & Scott Hansleman</h3>
                    <h4>Introduction</h4>
                    <p>
                        Azure AI Foundry is the AI factory and Scott Hanselman took the opportunity to record an episode of his podcast with Yina during 
                        the session. Azure AI Foundry is your AI app and agent factory to bring AI into apps for developers to take your idea to code and 
                        then to production. Hansleminutes podcast factory is an example, Hansleminutes is a podcast Scott has been doing for many years with 
                        almost one thousand episodes and almost five hundred hours of content. Scott's aim is to use AI in the parts that suck, such as show 
                        notes. Hansleminutes workflow is guest intake, which is a bunch of toil including collecting bios, socials, collecting release forms 
                        along with preparation, recording, editing, packaging such as exporting audio, writing show notes and generating transcripts plus 
                        publishing, promotion to generate social media copy or localisation and an archival step. The first step was to take the audio files 
                        to take transcriptions and take steps to erase toil at scale and still retain control without losing quality.
                    </p>
                    <h4>Agentic AI</h4>
                    <p>
                        Agentic AI to erase toil at scale including unstructured content glut such as audio, video and documents which can be hard to manually 
                        search and transcribe, and an agentic pattern can use multimodal and speech agents to auto ingest content and label. Repetitive knowledge 
                        packaging such as show notes, documentation and FAQs can be hard without AI as it is copy-paste drudgery with potential for style drift, 
                        but an agentic pattern can have LLM-powered compose agents with brand guardrails. Reference and link curation can be hard with scattered 
                        sources or lead to stale links but with AI retrieval augmented agents can pull fresh de-duplicated references. Guest & topic metadata 
                        upkeep is hard without AI due to hand-entered bios prone to errors but with AI a knowledge-graph agent can enrich and reuse data. Quality 
                        and compliance checks need spot-checking by humans but could use policy-enforcing guard-agents to catch issues early. Analytics and feedback 
                        loops can lead to siloed dashboards without AI but with AI can have observability agents that emit unified metrics and auto-tune prompts.
                    </p>
                    <h4>Foundry Models</h4>
                    <p>
                        Azure AI Foundry has many models with an explosion of foundational models which create new choices and opportunities for functionality for 
                        developers with over 11,000 models and Microsoft are bringing unified access and to be easily switch models in code. You can see what models 
                        are available in the model catalogue including announcements, leaderboards and then filter the models by features such as industry or 
                        capabilities. You can also use the Foundry Agent to help pick the correct model for your use case or see the leaderboards to see what the 
                        best models are based on benchmarks including quality vs cost, quality vs safety and quality vs throughput. 
                    </p>
                    <p>
                        You can also use your own data to evaluate data, or an AI generated data set to see what the models can offer. There is also model router 
                        which takes selection toil from hour head which supports OpenAI models but soon will support more models and you can try out models in the 
                        Chat playground. Ideally the cost needs to be as low as possible so you can fine tune the model based on some episodes to then process later 
                        episodes at a lower cost. Foundry Local can be used to run AI models locally on your own hardware and GPU to use information obtained online 
                        to then be processed locally to produce a bio for the podcast episode and do this work locally. 
                    </p>
                    <h4>Agents</h4>
                    <p>
                        What is an agent which can take input such as system events, user messages and agent messages from other agents it can then use an LLM, 
                        instructions or tools which can make tool calls for retrieval, actions or memory and then output can be agent messages or tool results. 
                        You can do process automation and have an agent which can act and make any API calls and can support multi modal inputs such as text, 
                        speech or images and can be invoked by other agents. 
                    </p>
                    <p>
                        Multi-agent orchestration allows you to not need to have a lot of functionality in one agent but create an agent that has specific 
                        functionality and can orchestrate them together to do a process or task, you can give one agent the abilities to another by connecting 
                        them and can have multi-agent workflows where can have a human in the loop. Hanselminutes podcast factory can have a voice-enabled 
                        orchestrator to do guest intake for agents to do guest sourcing, bio generation and scheduling and then for packaging with agents for 
                        transcript and show notes generation along with a link resolver agent plus for promotion with agents for social copy generation, content 
                        localisation and scheduling of an episode of the podcast. 
                    </p>
                    <p>
                        Build agents your way with platform integrations for an agentic flow with code handling every detail with infrastructure as a service with 
                        code handling every detail using Azure AI Infrastructure where can bring your own frameworks, platform as  a service using code with managed 
                        services using Azure AI Foundry and Foundry Agent Service and software as a service with drag-and-drop UI using Copilot Studio which is the 
                        instant agent runtime. Azure AI Foundry Agent Service enables you to create agents declaratively and use different sets of models and tools 
                        which delivers enterprise readiness for trust for data, networking and security and choice with model choice and tools for enterprise 
                        connectivity. 
                    </p>
                    <p>
                        Activities you can do with an agent includes Agentic Retrieval with Azure AI Search to ask questions of the large amount of content available. 
                        Agents can be built as needed which can include the content needed along with link verification to make sure these are correct. You can use 
                        Semantic Kernel to perform actions in an Agent including being able to call other agents in a workflow which is done in YAML but to understand 
                        this you can visualise this with a Mermaid diagram in Visual Studio Code. You can add knowledge to models including files, Bing Custom Search 
                        for grounding AI models to make sure that a chat bot is restricted and focus on what you need it to do, so you can keep a model small and 
                        ground it only in the knowledge you need.
                    </p>
                    <h4>Observability</h4>
                    <p>
                        Foundry Observability is a set of tools to support the entire development lifecycle aligned with your end-to-end workflow to power visibility, 
                        monitoring and optimisation across the entire AI development lifecycle.  When going into production you want o be able to continuously monitor 
                        your solution to generate traces and have capabilities to do debugging and hitting the right reliability level for your application with 
                        support for Open Telemetry rather than log files being parsed with a regular expression and be able to use existing observability systems 
                        such as .NET Aspire. You can evaluate relevance, intent resolution and even task adherence to make sure agents are only doing things they 
                        should be and make sure if agents go out to the web don't perform any actions based upon that that they shouldn't be doing. There's also 
                        integration with CI/CD pipelines and have any evaluations run and there also integration with application analytics for monitoring for 
                        total tokens, inference calls or any errors and can see quality improve over time if making any changes.
                    </p>
                    <p>
                        End-to-end security for your AI investments is important with Microsoft Purview for data, Microsoft Entra for authentication and authorisation 
                        along with Microsoft Intune and Microsoft Defender for security plus Microsoft Sentinel. Agents you create get specific identities with Entra 
                        where you can assign entitlements and do governance and you can send any data to Purview.
                    </p>
                    <h4>Conclusion</h4>
                    <p>
                        Hanselminutes podcast factory of agents can reduce workload to minutes using minimal customised fine-tuned models where just need to make any 
                        small corrections or changes. Azure AI Foundry can infuse applications like the podcast factory with AI behaviour and deliver a return on your 
                        effort and use AI to reduce toil and take away task you don't want to do and focus on the fun stuff. Companies are creating agents to make 
                        significant gains in their processes and create the future of AI with Azure AI Foundry.
                    </p>    
                    <h3>An Overview of Windows AI Foundry - Tucker Burns & Dian Hartono</h3>           
                    <h4>Introduction</h4>
                    <p>
                        Why customers want to use local AI as don't always want to run in the cloud but on client devices and we are at a turning point for local 
                        models and power comes with flexibility supporting a hybrid approach for the best of both worlds of client and cloud. There is a great need 
                        for local AI for privacy and security including governance which allows full control of user data, latency and performance are only possible 
                        without network latency and running models locally makes this viable and running models close to sensors and places without reliability 
                        internet but need high availability and not all applications need cloud scale and models can run in the background with newer hardware.
                    </p>
                    <p>
                        Windows AI Foundry has many capabilities and features and supports built in and third-party AI models for a versatile platform for AI models. 
                        Built-in AI APIs include natural language and vision and there are powerful tools for integrating these with ease along with customisation 
                        with LoRA for Phi Silica and Knowledge Retrieval to tailor capabilities to specific developer needs and the open-source ecosystem is being 
                        embraced with Foundry Local and Model Catalogue which makes it easy to use local models and Windows with SDKs and APIs to integrate them and 
                        can switch between local and cloud inference. Windows ML enables execution of AI models from CPU to GPU and NPU while minimising dependency 
                        management. Windows AI Foundry provided versatility and capabilities for first-party and third-party models on Windows.
                    </p>
                    <h4>Windows ML</h4>
                    <p>
                        You can bring your own model including PyTorch and ONNX, AI toolkit helps you convert, quantise and evaluate models, simplifies dependencies 
                        including execution providers, drivers and ONNX runtime out-of-the-box and is build for speed to easily scale AI workloads and provide native 
                        powerful Windows APIs along with reference documentation and sample code. Windows ML is the foundation on what Foundry Local is built on and 
                        provides developers a high degree of flexibility about what models to run or where and can leverage models, prepare models and use models 
                        within your applications.
                    </p>
                    <p>
                        Developer experience is bringing in PyTorch models with AI Toolkit for Visual Studio Code and can perform conversion and quantisation to create 
                        a hardware optimised model that can then use the Windows ML APIs using NPU, CPU or GPU execution providers. There is the AI Dev Gallery 
                        application with demos and examples that can show what you can do with Windows AI Foundry such as samples to classify an image, you can select 
                        a downloadable model you want to use or add an ONNX model from the file system. You can chose what device to run on from NPU, GPU or CPU or use 
                        smarter settings for efficiency, performance or power which will pick the right hardware for the requirements needed and in the future can pull 
                        information from the model file to determine where it should be run from available execution providers.
                    </p>
                    <h4>Windows AI APIs</h4>
                    <p>
                        AI Dev Gallery allows you to try out different capabilities and you can explore the Windows AI Apis such as generate text and you can try out an 
                        example or recognise text with OCR or can explore imaging APIs such as Image Super Resolution to scale images to a specific size and Image Segmentation 
                        which removes the background or Object Erase which will remove an object from an image. Image Description generates a description of an image and you 
                        can see an example in AI Dev Gallery and you can explore the API that powers this particular behaviour and it follows similar patterns as other APIs, 
                        it will check that the model feature is available and each API is different so there will be some differences when calling the model but you can copy 
                        the code and integrate it into the application or export the sample as a Visual Studio Project or try it out with your own images and explore the 
                        Windows AI APIs in AI Dev Gallery.
                    </p>
                    <p>
                        Windows AI APIs are powered by in-box models available on Copilot + PCs and are distributed via Windows Update, they are a level of abstraction from the 
                        models it is delivered for developers, so don't have to care about the exact model but only the capability that is being delivered as part of Windows App SDK. 
                        If you want to do more and customise on device models with Windows AI APIs customisation to make inbox models work better for your domain, for example with 
                        brand voice to make it sound like your own company by using the LoRA customisation path which can also be used for task adaptation to optimise or format 
                        for your own workflows or combined with knowledge retrieval to use domain-specific language such as legal, medical or technical terms or for knowledge 
                        retrieval to provide private knowledge grounding to answer based on y our documents and content. LoRA fine-tuning can nudge a model to particular tone 
                        or task of your domain or knowledge retrieval powered by semantic search to ground answers based on local knowledge.
                    </p>
                    <p>
                        LoRA Adapter developer flow starts with deciding the evaluation criteria, try prompt API and create a dataset for your specific scenario and 
                        then the rest is handled for you including to train an adapter, use the adapter for Phi Silica and evaluate the adapter. LoRA fine-tuning can 
                        be done via the Visual Studio Code AI Toolkit to fine-tine the Phi Silica model local on your device where you can create a project and which 
                        model to fine-tine, then select the training and test data set to create the adapter for any particular use cases and there are also other 
                        configurations that can be performed if needed. The jobs to fine-tune an AI model are done online using your Azure account in your own 
                        subscription and resources with any data and LoRA adapter is just yours. You can actually evaluate an adapter from within AI Dev Gallery 
                        with a prompt and will generate with and without an adapter to see if the adapter is behaving as needed and you don't need to know how to 
                        configure anything in detail to do this.
                    </p>
                    <p>
                        Knowledge retrieval is another way you can customise your experience by customising what the model knows, when a user queries this can search 
                        relevant model and use an LLM for a user query with contexts from the knowledge base for the query with top matching contexts. You don't need 
                        to embed the knowledge in the model but use the right data to feed into the prompt such as application data and user content which can be big, 
                        dynamic and constantly changing. You could have an application which provides the information from the application as the basis of the 
                        knowledge needed to go along with the prompt such as notes within a note application.
                    </p>
                    <p>
                        Windows AI APIs are now available in Windows App SDK with generally available or stable APIs including rewrite, image description, object erase, 
                        image segmentation, image super resolution, optical character recognition, text summary and text to table. Public preview or experimental APIs 
                        include LoRa for Phi Silica, Phi Silica prompt and conversation summary and private preview or private APIs include knowledge retrieval and 
                        semantic search. Windows AI experiences powered by Windows AI Foundry include Recall, Live Captions, Windows Studio Effects, Cocreator, 
                        Restyle Image, Super Resolution, Image Creator, Generative fill, improved Windows search and click to do. Web developers can take 
                        advantage of Web AI APIs available in Microsoft Edge including Prompt API, Text Rewrite, Text Summarisation, Text Write and coming soon 
                        Text Translate with many being proposed as web standards to be supported in many browsers.
                    </p>
                    <h4>Foundry Local</h4>
                    <p>
                        Foundry Local combines Windows AI Foundry with Azure AI Foundry to help evolve the platform of AI deployment which is now available in public preview. 
                        Azure AI Foundry provides models for cloud execution or download, and Windows AI Foundry brings local execution on GPU, CPU and NPU to provide easy to 
                        use flexible models to developers. Foundry local provides ready to use open source and other models pre-optimised for GPUs, CPUs and NPUs and has a built 
                        in command line interface to download and test models locally and model management service allows models to be distributed from model catalogues and as 
                        an open platform Windows also supports other model catalogues. Foundry local allows developers to browse models that available and relevant for your device 
                        and then you can then trigger a download of a model or make it ready for inference if it has already been downloaded. You can also see what models have 
                        already been downloaded onto your device which can then be ran, loaded into memory and can then submit a prompt and try out the model. 
                    </p>
                    <p>
                        It easy to work with Foundry local in a simple and intuitive way there is a REST Api and can be used with any SDK which is compatible with the OpenAI 
                        API specification which makes it easy to embrace the hybrid paradigm of local and cloud-based models. Managing and distributing models directly onto a 
                        local users' device makes it easy so you don't have to embed the models into your applications. When using the Foundry Local SDK, you can easily modify 
                        your code to hit a local endpoint instead of a cloud endpoint to leverage models. When multiple applications are using the same model only one version 
                        of the model needs to be downloaded with Foundry local which is embedded into Windows 11 and Windows App SDK. Windows AI Foundry provides task-based APIs 
                        and access to open-source, third-party or your own models to build AI into your own applications.
                    </p>
                    <h3>SQL Server 2025: The Database Developer Reimagined - Bob Ward, Muazma Zahid</h3>
                    <h4>Introduction</h4>
                    <p>
                        SQL Server 2025 infuses AI and has a new logo! Data is the fuel that powers AI, without data AI is pretty hard to achieve and now SQL Server 2025 is part 
                        of this. SQL Server 2025 is the AI-ready enterprise database from ground to cloud with best-in-class security and performance, AI built-in, made for developers 
                        and with Cloud agility through Azure. You will be able to use T-SQL and be able to choose the AI model you want, and it is more than AI as this is the most 
                        significant release of SQL Server in a decade.
                    </p>
                    <p>
                        SQL Server 2025 builds on a foundation of innovation. SQL Server 2017 enabled SQL Server on Linux with support for containers, adaptive query processing, 
                        automatic tuning, graph database and machine learning services. SQL Server 2019 added data virtualisation, intelligent query processing, accelerated database 
                        recovery and data classification. SQL Server 2022 was cloud connected and supported IQP NextGen, Ledger, Data Lakes and T-SQL enhancements. SQL Server 2025 
                        has innovations and enhancements from past releases, and you can develop once and deploy anywhere for SQL Server 2025, Azure SQL and SQL database in Fabric 
                        supporting T-SQL for developers, SQL engine, tools along with Fabric, AI and Copilots.
                    </p>
                    <p>
                        SQL Server 2025 has AI built-in and can develop modern data applications and integrate your data with Fabric, it is secure by default with a mission critical engine 
                        that can be connected with Arc and assisted by Copilots along with being a benchmark leader, optimised for the latest hardware and build for all platforms including 
                        Azure, Windows, Linux and Kubernetes.  SQL Server Management Studio 21 is based on Visual Studio 2022 64-bit and supports Copilot in public preview, Git and TFS along 
                        with a migration assistant and a most request feature of dark mode, it also features a new connection experience, query editor improvements and always encrypted assessment. 
                    </p>
                    <h4>AI Built-in</h4>
                    <p>
                        What problems are you trying to solve with AI? SQL Server 2025 delivers smarter searching on your existing text data, bringing in other documents or text for centralised 
                        vector searching, provides building blocks for intelligent assistants to connect with Retrieval Augmented Generation and AI agents along with being able to take advantage 
                        of AI in a secure and scalable fashion including co-locating data plus overcome complexity by using the familiar T-SQL language with extensions to enable AI capabilities.
                    </p>
                    <p>
                        Building scalable AI applications for agentic RAG with vector search built into SQL Server 2025 to store vectors and data together for consistency and search for most 
                        relevant data, operational RAG to retrieve the most semantically relevant data from your database and use this to ground Large Language Models for specific scenarios such 
                        as model context protocol situation and normal structured queries to allow Large Language Models to query structured data and take advantage of rich metadata and query 
                        optimisation.
                    </p>
                    <p>
                        Build enterprise AI-ready applications by building agentic RAG patterns inside the engine with a Vector Store with native vector data type for a column you store in the table 
                        and DiskANN index on top of this, model management for ability to declare model definitions to point to the model of your choice on ground or cloud using T-SQL, embeddings 
                        built in for text chunking and built-in multimodal embedded generations, simple semantic searching with vector distance (KNN) and vector search (ANN) based on the index 
                        along with supporting framework integrations including LangChain, Semantic Kernel and Entity Framework Core and will support OpenAI models and Ollama models.
                    </p>
                    <p>
                        SQL Server 2025 Vector Search has AI inferencing endpoints for a wide range of models that are possible they don't reside in the engine but will use REST APIs to talk to AI 
                        models of your choice in a secure manner. You will declare a model definition in T-SQL where you can choose from three different protocols for the API format from Azure OpenAI, 
                        OpenAI compatible and Ollama where it exists, you declare once and use multiple and you can create embeddings which will use these models and will be stored in your vector type 
                        which is a sequence of numbers that models use to understand your data. In your application you can send a natural language prompt and can create another embedding and can use 
                        vector search to find most similar results based on the prompt using your data, this can also use the new JSON data type for JSON data. If there is a model that isn't supported, 
                        you can use the sp_invoke_external_rest_endpoint to talk to any model endpoint for greater extensibility and multimodal and can talk to any REST endpoint.
                    </p>
                    <p>
                        Security, SQL and AI in SQL Server 2025 is you control all access with SQL security with roles, permissions and authentication. You also control which AI models to use, and these 
                        AI models can exist on ground or cloud, or a combination are isolated from SQL. You can use Role Level Security, TDE and Dynamic Data Masking, track everything with SQL Server 
                        auditing and there is a ledger for chat history and feedback if building a chat application. This can be a very secure process to use AI in your application when using SQL Server 2025 
                        and with SQL Server Management Studio and Copilot you can get assistance about your data including determining what embeddings are in your database such as supporting multiple languages, 
                        embeddings can be generated with T-SQL and can use what models you want to use either locally or on cloud.
                    </p>
                    <h4>Developers, Developers, Developers</h4>
                    <p>
                        SQL Server 2025 has the most developer features in a decade. SQL Server 2025 is built for developers to develop modern data applications there is a Data API builder which can convert 
                        any table or stored procedure into a GraphQL endpoint you can connect to in your app for efficient data applications. SQL Server 2025 supports JSON type, index and updated T-SQL functions 
                        along with support for Regular Expressions and other T-SQL functions. There is a new capability called Change Event Streaming where can consume log change events and there is a REST API 
                        to connect your data to any REST interface including GraphQL from T-SQL which is useful for AI or any downstream application you have access to that you can expose through REST. 
                        GitHub Copilot for SQL is now available in Visual Studio Code and there is a Python Driver coming for SQL Server.                        
                    </p>
                    <p>
                        GitHub Copilot for SQL has the context of your database including the schema so you can get the queries you need in T-SQL based on your tables, but you can also write code with the SQL 
                        context to get code to help build an application to connect with your database for example using Semantic Kernel. SQL Server 2025 introduces Standard Developer Edition which is free for 
                        development and test purposes with all the features and limits from the SQL Server 2025 Standard Edition on all platforms with no licence costs. 
                    </p>
                    <p>
                        JSON before SQL Server 2025 was varchar/nvarchar based and any index that supports this but in SQL Server 2025 there is a native json datatype for JSON using binary storage up to 2GB 
                        along with a json index and then functions to use when building applications and are brining NoSQL inside SQL as part of your queries and your code. You can use JSON_VALUE to get any 
                        value from a JSON document and there are new capabilities for aggregation with JSON_ARRAYAGG and there are key / value operations using JSON_OBJECTAGG. You can use modify to update 
                        individual JSON values for complex JSON payloads and do this as part of the SQL Server 2025 engine and can operate directly with values. To performantly query JSON you can create a 
                        json index so can then search a document using the index and you can see this in the execution plan from SQL Server Management Studio when using SQL Server 2025 with a json index. 
                        Additional T-SQL support includes regular expression functions, Base 64 functions, fuzzy string match functions, substring with optional length, string concatenation operator along 
                        with CURRENT_DATE to get the date and DATEADD which now supports bigint.
                    </p>
                    <p>
                        Capturing changes with SQL Server includes Change Tracking for cache invalidation and sync, Change Data Capture for data warehousing and auditing. SQL Server 2025 supports Change Event 
                        Streaming for event-driven architectures, microservice integrations, real-time analytics, cache sync and AI agents.  With Change Event Streaming in SQL Server 2025 you want to take an 
                        action based on a change in data from SQL Server which can scale as your data scales and is near-real time and is push based not pull based which can be enabled for your database and 
                        when setting up an event stream you specify the Azure Event Hub and table where changes will be pushed to and these are JSON-based in a format called cloud event which contains schema 
                        information and column names. You can use Azure Functions to take events from an Azure Event Hub to create an AI Agent and can even use GitHub Copilot to accept the event from the 
                        Event Hub to parse out the information including the data itself from the event triggered from Change Event Streaming and then pass this to an AI agent.
                    </p>
                    <p>
                        SQL Server 2025 is integrated with Microsoft Fabric which integrates your operation data with a unified data platform, Microsoft Fabric includes Data Factory, Real-Time Intelligences, 
                        Databases, Analytics, Industry Solutions, Power BI along with partner solutions. Mirroring from SQL Server 2019 and later replicates databases to Fabric with zero ETL and data I replicated 
                        to One Lake and is kept up to date in near real-time, mirroring protects operation databases from analytical queries and computer replication is included with your Fabric capacity for no 
                        cost and there is free mirroring storage for replicas tiered to Fabric Capacity.
                    </p>
                    <p>
                        Enhancing the industry engine in SQL Server 2025 includes many changes to Security including security cache improvements, authentication using system-assigned managed identity and backup 
                        to URL with managed identity. Performance improvements include optimised locking, change tracking cleanup and batch mode optimisations. Availability changes include improved health 
                        diagnostics, communication control flow tuning and DAG sync improvements. 
                    </p>
                    <p>
                        Mission Critical Engine for performance and availability of your modern database including improved concurrency with optimised locking, abort query hint and tempdb resource governance 
                        along with accelerating performance with IQP enhancements, columnstore indexes and query store on read replicas in a production environment and increase HADR with reliable failover for 
                        availability groups, availability group tuning and diagnostics plus backup enhancements. Transaction locks help avoid blocking with lock escalation and can be turned on in SQL Server 2025 
                        to prevent an entire table being locked with optimised locking.
                    </p>
                    <h4>Conclusion</h4>
                    <p>
                        SQL Server 2025 Platform Architecture is something new that supports new applications and new platforms to build optimised applications, AI agents and connect to Microsoft Fabric or use 
                        embeddings from Azure AI Foundry where AI agents can then use Azure EventHub to respond to events from the database or call external REST endpoints to include AI models locally or in 
                        the cloud as well as other online capabilities using REST APIs.
                    </p>
                    <h3>Elevating Development with .NET Aspire: AI, Cloud, and Beyond - Damian Edwards, David Fowler, Maddy Montaquila</h3>
                    <h4>What is Aspire?</h4>
                    <p>
                        Aspire allows you to build intelligent applications intelligently where you can bring together infusing AI into your .NET apps with Agentic Apps with Agentic DevOps which are AI-powered 
                        agents operating as a member of your development team. The dev loop kills velocity, and local dev is fragile with high friction onboarding with slow ramp up time, inconsistent dev 
                        experiences make things hard to troubleshoot and critical workflows rely on scripts, hacks and tribal knowledge. 
                    </p>
                    <p>
                        Aspire lets developers just be developers again where can onboard an intern in an afternoon instead of a couple of weeks, be able to run full integration suite locally without spinning 
                        up CI/CD or build machines, or with the dashboard able to test telemetry flow without waiting for a full deployment and don't have to be an expert in Bicep to be able to deploy 
                        applications. Aspire lets you develop again and can turn on and instrument things such as Open Telemetry, logging and resiliency with Polly including retries and be able to have 
                        your own standards and ship your own extension methods and have a deployment story that is improved.
                    </p>
                    <h4>Devis Lucato</h4>
                    <p>
                        Devis works on many AI projects in the office of the CTO including Semantic Kernel to make AI development more productive including developing SDKs and moving to encapsulating this 
                        into a single API. Were asked how to secure memory for a knowledge base with Kernel Memory and create a solution to inject and process data with a pipeline with a web service that 
                        can point any language to which allowed internal components to be changed so enabled many extensions to create a flexible pipeline but it was hard to set up but with Aspire it is 
                        very easy to spin things up locally and also be able to deploy to the cloud and Aspire is integrated into many IDEs including JetBrains Rider. All the resources will be spun up as 
                        needed rather than having to start each one manually when working locally and can also combine other resources such as those in Python and Node.js 
                    </p>
                    <h4>Aspire</h4>
                    <p>
                        Aspire enabled code-first control, is modular and extensible with observable from the start for flexible deployments and has been around for a year and there are over 130 integrations, 
                        over 700 community PRs and 70% of top .NET customers are using Aspire. Aspire with Copilot is an AI-assisted experience directly in the Aspire developer dashboard where it can identify 
                        resource issues, find bottlenecks and help with spotting problems at development time, you can ask Copilot about specific traces and find root causes of errors, and it works seamlessly 
                        with the rest of your Copilot agents in Visual Studio. 
                    </p>
                    <p>
                        Aspire is released every six weeks and things are being fixed and value is being added, and it is easy and low friction to update .NET Aspire. You can update the .NET templates if the 
                        .NET Aspire templates have been installed with “dotnet new update” from Command Line or Terminal to the latest version of the templates for .NET Aspire but there is work in progress to 
                        make the update experience work in Visual Studio. For a .NET Aspire project you can also use “dotnet outdated -inc Aspire -u” from the Command Line to update the .NET Aspire packages 
                        and you will also need to update the SDK version in the csproj in Visual Studio or Visual Studio Code and once updated you can take advantage of the latest features in .NET Aspire.
                    </p>
                    <h4>Your <s>Aspirations</s> Feedback</h4>    
                    <p>
                        .NET Developers understand features such as Project files etc but other developers won't understand this so Microsoft has aspiration to broaden the horizons of .NET Aspire but other 
                        developers have to get over the dotnetisms for this so there will be a command line experience for .NET Aspire with “aspire run” which is a .NET command line tool which takes normal 
                        things you do in Visual Studio but making these available in a Command Line Interface where you can create the templates and also trust the developer certificate automatically to make 
                        it easier, the point of this is to hide the default things that .NET Aspire has to do. You can also dee a small Dashboard-like experience in the Command Line Interface to show anything 
                        that has been started without needing to start Visual Studio or Visual Studio Code. You can also add additional .NET Aspire packages with “aspire add” to add these to your project and 
                        you can also add npm applications such as Vite + Vue.js.
                    </p>
                    <h4>Software Developer Lifecycle</h4>
                    <p>
                        Onboard, Develop, Test and Deploy focus of .NET Aspire has been Develop and Test, the Command Line Interface targets Onboard. With Deploy this has been revisited in regards to boosting 
                        productivity of teams in Microsoft teams using .NET Aspire to help them move faster who were using it for local development including Xbox and Copilot teams but many teams were asking 
                        for help to deploy and looked at building an experience for those teams to deploy using internal processes to then help everyone else to deploy to Azure and Azure Container Applications 
                        by adding support to be able to deploy everywhere.
                    </p>
                    <p>
                        The application host in .NET Aspire is a great place to see the structure of the application and see the application's configuration as you need to tell .NET Aspire what your application 
                        is including the services and how they talk to each other which can be visualised in the Resource table and graph in the .NET Aspire dashboard. From .NET Aspire you can see the traces for 
                        any requests between different parts of the application. From the AppHost you can add compute resources to model environments such as an Azure Container App and you can turn on various 
                        things you may need when the application is deployed into these environments, and the environment informs the target of where things should be by creating the relevant setup in Bicep 
                        after running infra synth for Azure.
                    </p>
                    <p>
                        There is also preview support for Azure App Service in .NET Aspire which creates the relevant setup in Bicep for this environment after running infra synth. If you want to split compute 
                        environments for example for front end and back end you can specify different compute environments to disambiguate them, when infra synth is executed there will be different ones for the 
                        different parts of the application, and with some hints in the App Host can infer the networking that may be required for the components of the application to be able to communicate with 
                        each other, targeting different things should be as simple as possible such as cross-cloud deployments and be able to glue these together and represent these from your App Host to help 
                        represent the deployments used by teams in Microsoft.
                    </p>
                </article>
            </div>
            <!-- /Article -->
        </section>
        <!-- /Section -->
    </main>
    <!-- /Main -->
    <!-- Footer -->
    <footer class="footer mt-auto">
        <div class="steps steps-articles"></div>
        <nav class="navbar bottom navbar-light p-2 bg-articles">
            <a href="../../"><img src="../../images/footer.png" alt="RoguePlanetoid" height="20px"></a>
            <a href="https://www.twitter.com/rogueplanetoid" target="_blank"><img src="../../images/twitter.png"
                    height="45px" alt="@RoguePlanetoid"></a>
        </nav>
    </footer>
    <!-- /Footer -->
    <!-- Scripts -->
    <script src="../../bootstrap/js/bootstrap.bundle.min.js"></script>
    <!-- /Scripts-->
</body>

</html>