<!doctype html>
<html lang="en" class="h-100">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=yes">
    <meta name="description" content="RoguePlanetoid" />
    <!-- Twitter -->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:site" content="@rogueplanetoid" />
    <meta name="twitter:creator" content="@rogueplanetoid" />
    <meta name="twitter:title" content="RoguePlanetoid" />
    <meta name="twitter:description" content="Posts, Articles and Podcast" />
    <meta name="twitter:image" content="https://www.rogueplanetoid.com/images/card.png" />
    <meta name="twitter:image:alt" content="Posts, Articles and Podcast" />
    <!-- /Twitter -->
    <!-- Open Graph -->
    <meta property="og:type" content="website" />
    <meta property="og:title" content="RoguePlanetoid" />
    <meta property="og:site_name" content="RoguePlanetoid">
    <meta property="og:description" content="Posts, Articles and Podcast" />
    <meta property="og:image" content="https://www.rogueplanetoid.com/images/card.png" />
    <meta property="og:image:width" content="1280" />
    <meta property="og:image:height" content="630" />
    <!-- /Open Graph -->
    <link rel="icon" href="../../favicon.ico" />
    <link href="../../css/style.css" rel="stylesheet">
    <link href="../../bootstrap/css/bootstrap.min.css" rel="stylesheet">
    <title>RoguePlanetoid - Articles - Microsoft Build 2025 - Sessions</title>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-8038JPY7YN"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-8038JPY7YN');
    </script>
</head>

<body class="d-flex flex-column h-100">
    <!-- Main -->
    <main>
        <!-- Navigation -->
        <nav class="navbar top navbar-light navbar-expand-md bg-brand">
            <div class="container-fluid">
                <button class="navbar-toggler" type="button" data-bs-toggle="collapse"
                    data-bs-target="#rogueplanetoid-navbar" aria-controls="rogueplanetoid-navbar" aria-expanded="false"
                    aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>
                <a class="navbar-brand" href="../../">
                    <img src="../../images/header-articles.png" alt="RoguePlanetoid" height="25px">
                </a>
                <div id="rogueplanetoid-navbar" class="collapse navbar-collapse">
                    <ul class="navbar-nav">
                        <li class="nav-item">
                            <a class="nav-link text-articles" href="../../">Home</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link text-articles" href="../../posts">Posts</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link text-articles" href="../../articles">Articles</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link btn btn-articles active" href="../microsoft-build-2025-sessions">Microsoft Build 2025 - Sessions</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link text-articles" href="../../podcast">Podcast</a>
                        </li>
                    </ul>
                </div>
                <a href="https://www.comentsys.com" target="_blank">
                    <img src="../../images/comentsys.png" alt="Comentsys" height="50px">
                </a>
            </div>
        </nav>
        <!-- /Navigation -->
        <!-- Section -->
        <section class="text-center text-black container">
            <!-- Article -->
            <div class="text-start">
                <article class="blog-post">
                    <h2 class="blog-post-title mb-1">Microsoft Build 2025 - Sessions</h2>
                    <p class="blog-post-meta">21st May 2025</p>
                    <img src="../assets/microsoft-build-2025.png" class="img-fluid" alt="Microsoft Build 2025 - Sessions" />
                    <h3>Azure AI Foundry: The AI app and Agent Factory - Yina Arenas & Scott Hansleman</h3>
                    <h4>Introduction</h4>
                    <p>
                        Azure AI Foundry is the AI factory and Scott Hanselman took the opportunity to record an episode of his podcast with Yina during 
                        the session. Azure AI Foundry is your AI app and agent factory to bring AI into apps for developers to take your idea to code and 
                        then to production. Hansleminutes podcast factory is an example, Hansleminutes is a podcast Scott has been doing for many years with 
                        almost one thousand episodes and almost five hundred hours of content. Scott's aim is to use AI in the parts that suck, such as show 
                        notes. Hansleminutes workflow is guest intake, which is a bunch of toil including collecting bios, socials, collecting release forms 
                        along with preparation, recording, editing, packaging such as exporting audio, writing show notes and generating transcripts plus 
                        publishing, promotion to generate social media copy or localisation and an archival step. The first step was to take the audio files 
                        to take transcriptions and take steps to erase toil at scale and still retain control without losing quality.
                    </p>
                    <h4>Agentic AI</h4>
                    <p>
                        Agentic AI to erase toil at scale including unstructured content glut such as audio, video and documents which can be hard to manually 
                        search and transcribe, and an agentic pattern can use multimodal and speech agents to auto ingest content and label. Repetitive knowledge 
                        packaging such as show notes, documentation and FAQs can be hard without AI as it is copy-paste drudgery with potential for style drift, 
                        but an agentic pattern can have LLM-powered compose agents with brand guardrails. Reference and link curation can be hard with scattered 
                        sources or lead to stale links but with AI retrieval augmented agents can pull fresh de-duplicated references. Guest & topic metadata 
                        upkeep is hard without AI due to hand-entered bios prone to errors but with AI a knowledge-graph agent can enrich and reuse data. Quality 
                        and compliance checks need spot-checking by humans but could use policy-enforcing guard-agents to catch issues early. Analytics and feedback 
                        loops can lead to siloed dashboards without AI but with AI can have observability agents that emit unified metrics and auto-tune prompts.
                    </p>
                    <h4>Foundry Models</h4>
                    <p>
                        Azure AI Foundry has many models with an explosion of foundational models which create new choices and opportunities for functionality for 
                        developers with over 11,000 models and Microsoft are bringing unified access and to be easily switch models in code. You can see what models 
                        are available in the model catalogue including announcements, leaderboards and then filter the models by features such as industry or 
                        capabilities. You can also use the Foundry Agent to help pick the correct model for your use case or see the leaderboards to see what the 
                        best models are based on benchmarks including quality vs cost, quality vs safety and quality vs throughput. 
                    </p>
                    <p>
                        You can also use your own data to evaluate data, or an AI generated data set to see what the models can offer. There is also model router 
                        which takes selection toil from hour head which supports OpenAI models but soon will support more models and you can try out models in the 
                        Chat playground. Ideally the cost needs to be as low as possible so you can fine tune the model based on some episodes to then process later 
                        episodes at a lower cost. Foundry Local can be used to run AI models locally on your own hardware and GPU to use information obtained online 
                        to then be processed locally to produce a bio for the podcast episode and do this work locally. 
                    </p>
                    <h4>Agents</h4>
                    <p>
                        What is an agent which can take input such as system events, user messages and agent messages from other agents it can then use an LLM, 
                        instructions or tools which can make tool calls for retrieval, actions or memory and then output can be agent messages or tool results. 
                        You can do process automation and have an agent which can act and make any API calls and can support multi modal inputs such as text, 
                        speech or images and can be invoked by other agents. 
                    </p>
                    <p>
                        Multi-agent orchestration allows you to not need to have a lot of functionality in one agent but create an agent that has specific 
                        functionality and can orchestrate them together to do a process or task, you can give one agent the abilities to another by connecting 
                        them and can have multi-agent workflows where can have a human in the loop. Hanselminutes podcast factory can have a voice-enabled 
                        orchestrator to do guest intake for agents to do guest sourcing, bio generation and scheduling and then for packaging with agents for 
                        transcript and show notes generation along with a link resolver agent plus for promotion with agents for social copy generation, content 
                        localisation and scheduling of an episode of the podcast. 
                    </p>
                    <p>
                        Build agents your way with platform integrations for an agentic flow with code handling every detail with infrastructure as a service with 
                        code handling every detail using Azure AI Infrastructure where can bring your own frameworks, platform as  a service using code with managed 
                        services using Azure AI Foundry and Foundry Agent Service and software as a service with drag-and-drop UI using Copilot Studio which is the 
                        instant agent runtime. Azure AI Foundry Agent Service enables you to create agents declaratively and use different sets of models and tools 
                        which delivers enterprise readiness for trust for data, networking and security and choice with model choice and tools for enterprise 
                        connectivity. 
                    </p>
                    <p>
                        Activities you can do with an agent includes Agentic Retrieval with Azure AI Search to ask questions of the large amount of content available. 
                        Agents can be built as needed which can include the content needed along with link verification to make sure these are correct. You can use 
                        Semantic Kernel to perform actions in an Agent including being able to call other agents in a workflow which is done in YAML but to understand 
                        this you can visualise this with a Mermaid diagram in Visual Studio Code. You can add knowledge to models including files, Bing Custom Search 
                        for grounding AI models to make sure that a chat bot is restricted and focus on what you need it to do, so you can keep a model small and 
                        ground it only in the knowledge you need.
                    </p>
                    <h4>Observability</h4>
                    <p>
                        Foundry Observability is a set of tools to support the entire development lifecycle aligned with your end-to-end workflow to power visibility, 
                        monitoring and optimisation across the entire AI development lifecycle.  When going into production you want o be able to continuously monitor 
                        your solution to generate traces and have capabilities to do debugging and hitting the right reliability level for your application with 
                        support for Open Telemetry rather than log files being parsed with a regular expression and be able to use existing observability systems 
                        such as .NET Aspire. You can evaluate relevance, intent resolution and even task adherence to make sure agents are only doing things they 
                        should be and make sure if agents go out to the web don't perform any actions based upon that that they shouldn't be doing. There's also 
                        integration with CI/CD pipelines and have any evaluations run and there also integration with application analytics for monitoring for 
                        total tokens, inference calls or any errors and can see quality improve over time if making any changes.
                    </p>
                    <p>
                        End-to-end security for your AI investments is important with Microsoft Purview for data, Microsoft Entra for authentication and authorisation 
                        along with Microsoft Intune and Microsoft Defender for security plus Microsoft Sentinel. Agents you create get specific identities with Entra 
                        where you can assign entitlements and do governance and you can send any data to Purview.
                    </p>
                    <h4>Conclusion</h4>
                    <p>
                        Hanselminutes podcast factory of agents can reduce workload to minutes using minimal customised fine-tuned models where just need to make any 
                        small corrections or changes. Azure AI Foundry can infuse applications like the podcast factory with AI behaviour and deliver a return on your 
                        effort and use AI to reduce toil and take away task you don't want to do and focus on the fun stuff. Companies are creating agents to make 
                        significant gains in their processes and create the future of AI with Azure AI Foundry.
                    </p>    
                    <h3>An Overview of Windows AI Foundry - Tucker Burns & Dian Hartono</h3>           
                    <h4>Introduction</h4>
                    <p>
                        Why customers want to use local AI as don't always want to run in the cloud but on client devices and we are at a turning point for local 
                        models and power comes with flexibility supporting a hybrid approach for the best of both worlds of client and cloud. There is a great need 
                        for local AI for privacy and security including governance which allows full control of user data, latency and performance are only possible 
                        without network latency and running models locally makes this viable and running models close to sensors and places without reliability 
                        internet but need high availability and not all applications need cloud scale and models can run in the background with newer hardware.
                    </p>
                    <p>
                        Windows AI Foundry has many capabilities and features and supports built in and third-party AI models for a versatile platform for AI models. 
                        Built-in AI APIs include natural language and vision and there are powerful tools for integrating these with ease along with customisation 
                        with LoRA for Phi Silica and Knowledge Retrieval to tailor capabilities to specific developer needs and the open-source ecosystem is being 
                        embraced with Foundry Local and Model Catalogue which makes it easy to use local models and Windows with SDKs and APIs to integrate them and 
                        can switch between local and cloud inference. Windows ML enables execution of AI models from CPU to GPU and NPU while minimising dependency 
                        management. Windows AI Foundry provided versatility and capabilities for first-party and third-party models on Windows.
                    </p>
                    <h4>Windows ML</h4>
                    <p>
                        You can bring your own model including PyTorch and ONNX, AI toolkit helps you convert, quantise and evaluate models, simplifies dependencies 
                        including execution providers, drivers and ONNX runtime out-of-the-box and is build for speed to easily scale AI workloads and provide native 
                        powerful Windows APIs along with reference documentation and sample code. Windows ML is the foundation on what Foundry Local is built on and 
                        provides developers a high degree of flexibility about what models to run or where and can leverage models, prepare models and use models 
                        within your applications.
                    </p>
                    <p>
                        Developer experience is bringing in PyTorch models with AI Toolkit for Visual Studio Code and can perform conversion and quantisation to create 
                        a hardware optimised model that can then use the Windows ML APIs using NPU, CPU or GPU execution providers. There is the AI Dev Gallery 
                        application with demos and examples that can show what you can do with Windows AI Foundry such as samples to classify an image, you can select 
                        a downloadable model you want to use or add an ONNX model from the file system. You can chose what device to run on from NPU, GPU or CPU or use 
                        smarter settings for efficiency, performance or power which will pick the right hardware for the requirements needed and in the future can pull 
                        information from the model file to determine where it should be run from available execution providers.
                    </p>
                    <h4>Windows AI APIs</h4>
                    <p>
                        AI Dev Gallery allows you to try out different capabilities and you can explore the Windows AI Apis such as generate text and you can try out an 
                        example or recognise text with OCR or can explore imaging APIs such as Image Super Resolution to scale images to a specific size and Image Segmentation 
                        which removes the background or Object Erase which will remove an object from an image. Image Description generates a description of an image and you 
                        can see an example in AI Dev Gallery and you can explore the API that powers this particular behaviour and it follows similar patterns as other APIs, 
                        it will check that the model feature is available and each API is different so there will be some differences when calling the model but you can copy 
                        the code and integrate it into the application or export the sample as a Visual Studio Project or try it out with your own images and explore the 
                        Windows AI APIs in AI Dev Gallery.
                    </p>
                    <p>
                        Windows AI APIs are powered by in-box models available on Copilot + PCs and are distributed via Windows Update, they are a level of abstraction from the 
                        models it is delivered for developers, so don't have to care about the exact model but only the capability that is being delivered as part of Windows App SDK. 
                        If you want to do more and customise on device models with Windows AI APIs customisation to make inbox models work better for your domain, for example with 
                        brand voice to make it sound like your own company by using the LoRA customisation path which can also be used for task adaptation to optimise or format 
                        for your own workflows or combined with knowledge retrieval to use domain-specific language such as legal, medical or technical terms or for knowledge 
                        retrieval to provide private knowledge grounding to answer based on y our documents and content. LoRA fine-tuning can nudge a model to particular tone 
                        or task of your domain or knowledge retrieval powered by semantic search to ground answers based on local knowledge.
                    </p>
                    <p>
                        LoRA Adapter developer flow starts with deciding the evaluation criteria, try prompt API and create a dataset for your specific scenario and 
                        then the rest is handled for you including to train an adapter, use the adapter for Phi Silica and evaluate the adapter. LoRA fine-tuning can 
                        be done via the Visual Studio Code AI Toolkit to fine-tine the Phi Silica model local on your device where you can create a project and which 
                        model to fine-tine, then select the training and test data set to create the adapter for any particular use cases and there are also other 
                        configurations that can be performed if needed. The jobs to fine-tune an AI model are done online using your Azure account in your own 
                        subscription and resources with any data and LoRA adapter is just yours. You can actually evaluate an adapter from within AI Dev Gallery 
                        with a prompt and will generate with and without an adapter to see if the adapter is behaving as needed and you don't need to know how to 
                        configure anything in detail to do this.
                    </p>
                    <p>
                        Knowledge retrieval is another way you can customise your experience by customising what the model knows, when a user queries this can search 
                        relevant model and use an LLM for a user query with contexts from the knowledge base for the query with top matching contexts. You don't need 
                        to embed the knowledge in the model but use the right data to feed into the prompt such as application data and user content which can be big, 
                        dynamic and constantly changing. You could have an application which provides the information from the application as the basis of the 
                        knowledge needed to go along with the prompt such as notes within a note application.
                    </p>
                    <p>
                        Windows AI APIs are now available in Windows App SDK with generally available or stable APIs including rewrite, image description, object erase, 
                        image segmentation, image super resolution, optical character recognition, text summary and text to table. Public preview or experimental APIs 
                        include LoRa for Phi Silica, Phi Silica prompt and conversation summary and private preview or private APIs include knowledge retrieval and 
                        semantic search. Windows AI experiences powered by Windows AI Foundry include Recall, Live Captions, Windows Studio Effects, Cocreator, 
                        Restyle Image, Super Resolution, Image Creator, Generative fill, improved Windows search and click to do. Web developers can take 
                        advantage of Web AI APIs available in Microsoft Edge including Prompt API, Text Rewrite, Text Summarisation, Text Write and coming soon 
                        Text Translate with many being proposed as web standards to be supported in many browsers.
                    </p>
                    <h4>Foundry Local</h4>
                    <p>
                        Foundry Local combines Windows AI Foundry with Azure AI Foundry to help evolve the platform of AI deployment which is now available in public preview. 
                        Azure AI Foundry provides models for cloud execution or download, and Windows AI Foundry brings local execution on GPU, CPU and NPU to provide easy to 
                        use flexible models to developers. Foundry local provides ready to use open source and other models pre-optimised for GPUs, CPUs and NPUs and has a built 
                        in command line interface to download and test models locally and model management service allows models to be distributed from model catalogues and as 
                        an open platform Windows also supports other model catalogues. Foundry local allows developers to browse models that available and relevant for your device 
                        and then you can then trigger a download of a model or make it ready for inference if it has already been downloaded. You can also see what models have 
                        already been downloaded onto your device which can then be ran, loaded into memory and can then submit a prompt and try out the model. 
                    </p>
                    <p>
                        It easy to work with Foundry local in a simple and intuitive way there is a REST Api and can be used with any SDK which is compatible with the OpenAI 
                        API specification which makes it easy to embrace the hybrid paradigm of local and cloud-based models. Managing and distributing models directly onto a 
                        local users' device makes it easy so you don't have to embed the models into your applications. When using the Foundry Local SDK, you can easily modify 
                        your code to hit a local endpoint instead of a cloud endpoint to leverage models. When multiple applications are using the same model only one version 
                        of the model needs to be downloaded with Foundry local which is embedded into Windows 11 and Windows App SDK. Windows AI Foundry provides task-based APIs 
                        and access to open-source, third-party or your own models to build AI into your own applications.
                    </p>
                </article>
            </div>
            <!-- /Article -->
        </section>
        <!-- /Section -->
    </main>
    <!-- /Main -->
    <!-- Footer -->
    <footer class="footer mt-auto">
        <div class="steps steps-articles"></div>
        <nav class="navbar bottom navbar-light p-2 bg-articles">
            <a href="../../"><img src="../../images/footer.png" alt="RoguePlanetoid" height="20px"></a>
            <a href="https://www.twitter.com/rogueplanetoid" target="_blank"><img src="../../images/twitter.png"
                    height="45px" alt="@RoguePlanetoid"></a>
        </nav>
    </footer>
    <!-- /Footer -->
    <!-- Scripts -->
    <script src="../../bootstrap/js/bootstrap.bundle.min.js"></script>
    <!-- /Scripts-->
</body>

</html>